{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "A convolutional neural network (CNN) is a a specific type of neural net that can be used to process data with a known, grid-like topology (e.g. 2D grid of pixel values). CNNs got popular due to their excellent performance in image recognition tasks. In this notebook, we will use a basic CNN to classify the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from mnist import MNIST\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset\n",
    "\n",
    "The MNIST dataset contains handwritten digits and is available [here](http://yann.lecun.com/exdb/mnist/). To proceed you first have to download the training data and labels and the test data and labels. Next, unpack them into a folder named \"mnist\" in your home directory. To make the data extraction work, you will have to rename the dots to -, for example t10k-images.idx3-ubyte must be renamed to t10k-images-idx3-ubyte.\n",
    "\n",
    "To read the MNIST files we will use the [python mnist package](https://pypi.org/project/python-mnist/). You can install the package via\n",
    "\n",
    "```bash\n",
    "pip install python-mnist\n",
    "```\n",
    "\n",
    "Note: another convenient way to download and handle the MNIST dataset is using torchvision. Torchvision has data loaders for common datasets such as Imagenet, CIFAR10, etc. If you need an example on how to use torchvision look [here](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (60000, 784)\n",
      "Training labels shape: (60000,)\n",
      "Test set shape: (10000, 784)\n",
      "Test set shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "path_to_data = os.path.expanduser(\"~/mnist/\")\n",
    "mnist = MNIST(path_to_data)\n",
    "\n",
    "x_train, y_train = mnist.load_training()\n",
    "x_test, y_test = mnist.load_testing()\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(f\"Training set shape: {x_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "\n",
    "print(f\"Test set shape: {x_test.shape}\")\n",
    "print(f\"Test set shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(2, 2, i + 1)\n",
    "    ax.imshow(np.reshape(x_train[i], (28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.from_numpy(x_train)\n",
    "x_test_tensor = torch.from_numpy(x_test)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_test_tensor = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN layers\n",
    "\n",
    "A convolutional neural network consists of a stack of layers. The most important layer types are: convolutional layer, pooling layer and fully connected layer. Typically, convolutional and pooling layers are used in an alternating fashion throughout the CNN. The final, fully connected layer is used to classify the extracted feature values into distinct classes.\n",
    "\n",
    "### Convolutional layer:\n",
    "\n",
    "A convolutional layer is a specific type of neural network layer that performs *convolutions* on an input image. The convolutions are performed by applying a certain number of convolutional filters (also called kernels) to the image. These filters are able to extract features from the input image (e.g. edges or lines). For each input region the kernel is applied to, a single value in the output feature map is produced. \n",
    "\n",
    "Detailed information about the functioning of convolutional layers and several illustrations can be found in the lecture notes of the [Stanford class on CNN's](http://cs231n.github.io/convolutional-networks/).\n",
    "\n",
    "\n",
    "**Output shape of a convolutional layer:**   \n",
    "To compute the output shape of a conv layer you need to know how a convolutional layer is structured. In summary (see Stanford class for details):\n",
    "\n",
    "A convolutional layer:\n",
    "- Accepts an input tensor of shape $W_1 \\times H_1 \\times D_1$ (width, height, depth)\n",
    "    - The depth describes the number of color channels. For example, for MNIST we have only 1 color channel (grayscale). Most images will be in RGB mode with 3 color channels (red, blue, green)\n",
    "     \n",
    "- Requires four hyperparameters:\n",
    "    - the number of filters $K$\n",
    "    - the shape / spacial extend of the filters $F$\n",
    "    - the stride $S$\n",
    "    - the amount of zero padding $P$   \n",
    "    common settings for padding are 'same' and 'valid'. 'same' means that the input will be padded with enough zeros to ensure that the output will have the same widht and height as the input. 'valid' means that no padding is used\n",
    "     \n",
    "- Produces an output tensor of shape  $W_2 \\times H_2 \\times D_2$\n",
    "    These shapes can be computed as follows:   \n",
    "    $W_2 = \\frac{W_1 - F + 2P}{S} + 1$   \n",
    "       \n",
    "    $H_2 = \\frac{H_1 - F + 2P}{S} + 1$\n",
    "   \n",
    "    $D_2 = K$\n",
    "\n",
    "\n",
    "**Example:**\n",
    "\n",
    "For MNIST, the input has shape $28 \\times 28 \\times 1$. In the first convolutional layer we will use $32$ filters with a kernel size of $5 \n",
    "\\times 5$. We will use \"same\" padding and a stride of $1$.\n",
    "\n",
    "Therefore, the output of the first convolutional layer will have the following shape: \n",
    "\n",
    "$W_2 = \\frac{28 - 5 + 2 * 0}{1} + 1 = 24$   \n",
    "       \n",
    "$H_2 = \\frac{28 - 5 + 2 * 0}{1} + 1 = 24$\n",
    "   \n",
    "$D_2 = K = 1$\n",
    "   \n",
    "    \n",
    "\n",
    "### Pooling layer:\n",
    "\n",
    "Pooling layers are used to reduce the dimensionality of the feature maps produced by convolutional layers. A pooling function replaces the output of a convolutional layer at a certain location with a summary statistic of the nearby outputs. For example, *max pooling* computes the *maximum* output value within a rectangular neighborhood.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "![title](figures/max_pool.png)\n",
    "\n",
    "\n",
    "### Fully connected layer:\n",
    "\n",
    "The fully connected layers are used to classify the features extracted by the convolutional and pooling layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "n_epochs = 10\n",
    "height = width = 28\n",
    "n_classes = 10\n",
    "\n",
    "n_train_samples = x_train.shape[0]\n",
    "n_test_samples = x_test.shape[0]\n",
    "\n",
    "n_train_batches = n_train_samples // batch_size\n",
    "n_test_batches = n_test_samples // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN network architecture\n",
    "\n",
    "Parts of this code were inspired by the official [Pytorch tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # 1 input image channel (= number of color channels)\n",
    "        # 32 filters/output channels\n",
    "        # kernel size 5x5\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(7 * 7 * 64, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling with stride 2 and kernel size 2\n",
    "        out = self.pool1(self.relu(self.conv1(x)))\n",
    "        out = self.pool2(self.relu(self.conv2(out)))\n",
    "        \n",
    "        # Flatten the output\n",
    "        out = out.view(-1, 7 * 7 * 64)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Linear(in_features=3136, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Current minibatch loss: 23.885\n",
      "Current minibatch loss: 0.177\n",
      "Current minibatch loss: 0.084\n",
      "\n",
      "Epoch: 1\n",
      "Current minibatch loss: 0.095\n",
      "Current minibatch loss: 0.076\n",
      "Current minibatch loss: 0.071\n",
      "\n",
      "Epoch: 2\n",
      "Current minibatch loss: 0.087\n",
      "Current minibatch loss: 0.049\n",
      "Current minibatch loss: 0.079\n",
      "\n",
      "Epoch: 3\n",
      "Current minibatch loss: 0.038\n",
      "Current minibatch loss: 0.020\n",
      "Current minibatch loss: 0.043\n",
      "\n",
      "Epoch: 4\n",
      "Current minibatch loss: 0.030\n",
      "Current minibatch loss: 0.033\n",
      "Current minibatch loss: 0.090\n",
      "\n",
      "Epoch: 5\n",
      "Current minibatch loss: 0.100\n",
      "Current minibatch loss: 0.032\n",
      "Current minibatch loss: 0.020\n",
      "\n",
      "Epoch: 6\n",
      "Current minibatch loss: 0.013\n",
      "Current minibatch loss: 0.025\n",
      "Current minibatch loss: 0.014\n",
      "\n",
      "Epoch: 7\n",
      "Current minibatch loss: 0.015\n",
      "Current minibatch loss: 0.010\n",
      "Current minibatch loss: 0.042\n",
      "\n",
      "Epoch: 8\n",
      "Current minibatch loss: 0.065\n",
      "Current minibatch loss: 0.053\n",
      "Current minibatch loss: 0.024\n",
      "\n",
      "Epoch: 9\n",
      "Current minibatch loss: 0.041\n",
      "Current minibatch loss: 0.012\n",
      "Current minibatch loss: 0.014\n",
      "\n",
      "Optimization done!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "\n",
    "    print()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i in range(n_train_batches):           \n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "\n",
    "        x_batch = x_train_tensor[start:end]        \n",
    "        y_batch = y_train_tensor[start:end]\n",
    "        \n",
    "        # Reshape the tensors and adapt their types\n",
    "        x_batch = x_batch.view([batch_size, 1, height, width])\n",
    "        x_batch = x_batch.type(torch.FloatTensor)\n",
    "        y_batch = y_batch.type(torch.LongTensor)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = cnn(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # update step\n",
    "        optimizer.step()        \n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            print(f\"Current minibatch loss: {loss.item():.3f}\")\n",
    "            \n",
    "print()\n",
    "print(f\"Optimization done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on test set: 0.08207326166359451\n",
      "Accuracy on test set: 98%\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "acc = []\n",
    "\n",
    "for i in range(n_test_batches):\n",
    "\n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size\n",
    "\n",
    "    x_test_batch = x_test_tensor[start:end]        \n",
    "    y_test_batch = y_test_tensor[start:end]\n",
    "    \n",
    "    x_test_batch = x_test_batch.view([batch_size, 1, height, width])\n",
    "    x_test_batch = x_test_batch.type(torch.FloatTensor)\n",
    "    y_test_batch = y_test_batch.type(torch.LongTensor)\n",
    "\n",
    "    outputs = cnn(x_test_batch)\n",
    "    loss = float(criterion(outputs, y_test_batch))\n",
    "\n",
    "    # compute accuracy\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    equal = np.equal(predicted, y_test_batch).sum()\n",
    "    accuracy = int(equal) / batch_size\n",
    "\n",
    "    losses.append(loss)\n",
    "    acc.append(accuracy)\n",
    "\n",
    "print(f\"Average loss on test set: {np.mean(losses)}\")\n",
    "print(f\"Accuracy on test set: {int(np.mean(acc) * 100)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
